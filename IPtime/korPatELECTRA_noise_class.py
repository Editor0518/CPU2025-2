import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, ElectraModel, AutoConfig
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
from tqdm import tqdm
import time

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# ì„¤ì •ê°’
ACCESS_TOKEN_PATH = "access_token.txt"
MODEL_FOLDER = "./electra"
REMOTE_MODEL_NAME = "KIPI-ai/KorPatElectra"
DATA_PATH = "patent_data_finalN.csv"
MAX_SEQ_LEN = 192
BATCH_SIZE = 16
EPOCHS = 10
LR = 1e-5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì•¡ì„¸ìŠ¤ í† í°
with open(ACCESS_TOKEN_PATH, "r") as f:
    access_token = f.read().strip()

# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
def load_tokenizer_and_model():
    if os.path.exists(MODEL_FOLDER):
        print("ğŸ” ë¡œì»¬ ELECTRA ëª¨ë¸ ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_FOLDER)
        config = AutoConfig.from_pretrained(MODEL_FOLDER)
        base_model = ElectraModel.from_pretrained(MODEL_FOLDER, config=config)
    else:
        print("ğŸŒ Hugging Faceì—ì„œ ELECTRA ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(REMOTE_MODEL_NAME, use_auth_token=access_token)
        config = AutoConfig.from_pretrained(REMOTE_MODEL_NAME, use_auth_token=access_token)
        base_model = ElectraModel.from_pretrained(REMOTE_MODEL_NAME, config=config, use_auth_token=access_token)
    return tokenizer, base_model, config

tokenizer, base_model, config = load_tokenizer_and_model()

# ===== ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° =====
df = pd.read_csv(DATA_PATH, encoding="cp949")
df = df[df['label'].notnull()].copy()
df["text"] = df["title"].fillna('') + " " + df["korean_summary"].fillna('')

# ===== ë¼ë²¨ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€ê²½ (Noise vs Non-Noise) =====
df['label'] = df['label'].apply(lambda x: 'N' if x == 'N' else 'non-N')
label_list = ['non-N', 'N']

# ===== ë¼ë²¨ ë¶„í¬ ë¶„ì„ =====
print("\nğŸ“Š ì „ì²´ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ë¶„ì„")
label_counts = df['label'].value_counts().sort_index()
for label, count in label_counts.items():
    print(f"  {label}: {count}ê°œ ({(count / len(df)) * 100:.1f}%)")

# ===== ë°ì´í„° í•„í„°ë§ =====
insufficient_labels = label_counts[label_counts < 2].index.tolist()
sufficient_labels = label_counts[label_counts >= 2].index.tolist()

if insufficient_labels:
    print(f"\nâŒ ì œì™¸í•  ë¼ë²¨: {insufficient_labels}")
    df = df[df['label'].isin(sufficient_labels)].copy()

label_counts = df['label'].value_counts()
valid_labels = label_counts[label_counts >= 2].index
df_filtered = df[df['label'].isin(valid_labels)].copy()

# ë¼ë²¨ ì¸ì½”ë”© (ì´ì§„)
labels = sorted(df_filtered['label'].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for label, i in label2id.items()}
df_filtered['label_id'] = df_filtered['label'].map(label2id)

# ğŸ”¹ 1ì°¨ ë¶„í• : train vs temp (val+test)
train_df, temp_df = train_test_split(
    df_filtered,
    test_size=0.2,
    stratify=df_filtered['label_id'],
    random_state=42
)

label_counts_temp = temp_df['label'].value_counts()
valid_labels_temp = label_counts_temp[label_counts_temp >= 2].index
temp_df = temp_df[temp_df['label'].isin(valid_labels_temp)].copy()
temp_df['label_id'] = temp_df['label'].map(label2id)

min_count = temp_df['label_id'].value_counts().min()
assert min_count >= 2, "âŒ stratifyë¥¼ ìœ„í•´ ê° ë¼ë²¨ì´ ìµœì†Œ 2ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."

# ğŸ”¹ 2ì°¨ ë¶„í• : val vs test
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    stratify=temp_df['label_id'],
    random_state=42
)

# ===== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ =====
class PatentDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=MAX_SEQ_LEN):
        self.texts = df['text'].tolist()
        self.labels = df['label_id'].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx], max_length=self.max_len,
            padding='max_length', truncation=True, return_tensors='pt')
        item = {k: v.squeeze(0) for k, v in encoding.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

train_loader = DataLoader(PatentDataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(PatentDataset(val_df, tokenizer), batch_size=BATCH_SIZE)
test_loader = DataLoader(PatentDataset(test_df, tokenizer), batch_size=BATCH_SIZE)

# ===== ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ =====
class ElectraClassifier(nn.Module):
    def __init__(self, electra_model, num_labels):
        super().__init__()
        self.electra = electra_model
        self.classifier = nn.Linear(self.electra.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)
        cls_vector = outputs.last_hidden_state[:, 0, :]
        return self.classifier(cls_vector)

model = ElectraClassifier(base_model, len(label2id)).to(DEVICE)
optimizer = AdamW(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss()

def train_epoch(model, dataloader, optimizer, criterion):
    model.train()
    total_loss, correct, total = 0, 0, 0
    step_times = []

    progress_bar = tqdm(dataloader, desc="Training", ncols=100)
    for batch in progress_bar:
        start = time.time()
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        batch_loss = loss.item()
        total_loss += batch_loss
        preds = torch.argmax(outputs, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

        step_time = time.time() - start
        step_times.append(step_time)

        speed = 1 / step_time if step_time > 0 else 0
        progress_bar.set_postfix({
            "loss": f"{total_loss / (total / labels.size(0)):.4f}",
            "accuracy": f"{correct / total:.4f}",
            "it/s": f"{speed:.1f}"
        })

    avg_step_time = sum(step_times) / len(step_times)
    return total_loss / len(dataloader), correct / total, avg_step_time

def eval_epoch(model, dataloader, criterion, phase="Validation"):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    step_times = []

    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc=phase, ncols=100)
        for batch in progress_bar:
            start = time.time()

            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            labels = batch['labels'].to(DEVICE)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(outputs, labels)

            batch_loss = loss.item()
            total_loss += batch_loss
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            step_time = time.time() - start
            step_times.append(step_time)

            speed = 1 / step_time if step_time > 0 else 0
            progress_bar.set_postfix({
                "loss": f"{total_loss / (total / labels.size(0)):.4f}",
                "accuracy": f"{correct / total:.4f}",
                "it/s": f"{speed:.1f}"
            })

    avg_step_time = sum(step_times) / len(step_times)
    return total_loss / len(dataloader), correct / total, avg_step_time

# ===== í•™ìŠµ ë£¨í”„ =====
print("_________________________________________________________________")
print("Total params:", sum(p.numel() for p in model.parameters()))
print("Trainable params:", sum(p.numel() for p in model.parameters() if p.requires_grad))
print("Non-trainable params:", sum(p.numel() for p in model.parameters() if not p.requires_grad))
print("_________________________________________________________________")

best_val_loss = float('inf')
best_val_acc = 0
patience, early_stop_count = 3, 0

for epoch in range(1, EPOCHS + 1):
    start_time = time.time()

    train_loss, train_acc, train_step_time = train_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_acc, val_step_time = eval_epoch(model, val_loader, criterion)
    epoch_time = time.time() - start_time

    print(f"\nEpoch {epoch}/{EPOCHS}")
    print(f"{len(train_loader)}/{len(train_loader)} [==============================] - "
          f"{int(epoch_time)}s {int(train_step_time)}s/step - accuracy: {train_acc:.4f} - loss: {train_loss:.4f} - "
          f"val_accuracy: {val_acc:.4f} - val_loss: {val_loss:.4f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_val_acc = val_acc
        early_stop_count = 0
        torch.save(model.state_dict(), "korpat_electra_noise_classifier.pth")
    else:
        early_stop_count += 1
        if early_stop_count >= patience:
            print(f"\nEarly stopping at epoch {epoch}.")
            break

# ===== í‰ê°€ =====
model.load_state_dict(torch.load("korpat_electra_noise_classifier.pth", weights_only=True))
test_loss, test_acc, _ = eval_epoch(model, test_loader, criterion, phase="Test")
print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼: Accuracy={test_acc:.4f}, Loss={test_loss:.4f}")
