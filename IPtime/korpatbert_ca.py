# korpatbert_classifier.py

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

import pandas as pd
import numpy as np
from tqdm import tqdm
from korpat_tokenizer import Tokenizer  # PDF ì œê³µ íŒŒì¼
import tensorflow as tf
from tensorflow import keras
#from keras import Dense
from tensorflow.keras.callbacks import EarlyStopping
import bert  # pip install bert-for-tf2


# ===== ê²½ë¡œ ë° ì„¤ì • =====
config_path = "./pretrained/korpat_bert_config.json"
vocab_path = "./pretrained/korpat_vocab.txt"
checkpoint_path = "./pretrained/model.ckpt-381250"

csv_path = "patent_data_finalN.csv"

MAX_SEQ_LEN = 192
BATCH_SIZE = 8
EPOCHS = 10
LR = 1e-5

# ===== KorPat Tokenizer ì„ ì–¸ =====
tokenizer = Tokenizer(vocab_path=vocab_path, cased=True)

# ===== ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° =====
df = pd.read_csv(csv_path, encoding="cp949")
df = df[df['label'].notnull()].copy()
df["text"] = df["title"].fillna('') + " " + df["korean_summary"].fillna('')


# ===== ì‚¬ìš©ì ì •ì˜ ë¼ë²¨ =====
label_list = [
    'AAA', 'AAB', 'AAC',
    'ABA', 'ABB', 'ABC',
    'ACA', 'ACB', 'ACC',
    'ADA', 'ADB', 'ADC', 'ADD',
    'AEA', 'AEB', 'AEC', 'AED',
    'N'  # ë…¸ì´ì¦ˆ
]

# ===== ë¼ë²¨ ë¶„í¬ ë¶„ì„ =====
print("ğŸ“Š ì „ì²´ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ë¶„ì„")
print("=" * 50)

# ì „ì²´ ë¼ë²¨ ë¶„í¬ í™•ì¸
label_counts = df['label'].value_counts().sort_index()
print(f"ì´ ë°ì´í„° ìˆ˜: {len(df)}")
print(f"ê³ ìœ  ë¼ë²¨ ìˆ˜: {len(label_counts)}")
print("\në¼ë²¨ë³„ ë°ì´í„° ìˆ˜:")
for label, count in label_counts.items():
    percentage = (count / len(df)) * 100
    print(f"  {label}: {count}ê°œ ({percentage:.1f}%)")

# ===== ë°ì´í„° í•„í„°ë§ (ìµœì†Œ 2ê°œ ì´ìƒ ë¼ë²¨ë§Œ ìœ ì§€) =====
print("\nğŸ” ë¼ë²¨ í•„í„°ë§ (ìµœì†Œ 2ê°œ ì´ìƒ)")
print("=" * 30)

# 2ê°œ ë¯¸ë§Œì¸ ë¼ë²¨ ì°¾ê¸°
insufficient_labels = label_counts[label_counts < 2].index.tolist()
sufficient_labels = label_counts[label_counts >= 2].index.tolist()

if insufficient_labels:
    print(f"âŒ ì œì™¸í•  ë¼ë²¨ (1ê°œ): {insufficient_labels}")
    print(f"âœ… ì‚¬ìš©í•  ë¼ë²¨ ({len(sufficient_labels)}ê°œ): {sufficient_labels}")
    
    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ë¼ë²¨ë§Œ í•„í„°ë§
    df_filtered = df[df['label'].isin(sufficient_labels)].copy()
    print(f"\ní•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(df)} â†’ {len(df_filtered)}")
else:
    print("âœ… ëª¨ë“  ë¼ë²¨ì´ 2ê°œ ì´ìƒì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
    df_filtered = df.copy()

# ===== ìµœì¢… ë¼ë²¨ ë§µí•‘ =====
# í•„í„°ë§ëœ ë°ì´í„°ì˜ ë¼ë²¨ë§Œ ì‚¬ìš©
labels = sorted(df_filtered["label"].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for label, i in label2id.items()}
df_filtered["label_id"] = df_filtered["label"].map(label2id)

for i, label in enumerate(labels):
    count = len(df_filtered[df_filtered['label'] == label])

# ===== ë°ì´í„° ë¶„ë¦¬ (ì´ì œ ì•ˆì „í•˜ê²Œ stratify ì‚¬ìš© ê°€ëŠ¥) =====
print(f"\nğŸ”„ ë°ì´í„° ë¶„ë¦¬ ì‹œì‘...")
from sklearn.model_selection import train_test_split

try:
    train_df, test_df = train_test_split(
        df_filtered, 
        test_size=0.1, 
        stratify=df_filtered["label_id"], 
        random_state=42
    )
    
    train_df, val_df = train_test_split(
        train_df, 
        test_size=0.1, 
        stratify=train_df["label_id"], 
        random_state=42
    )
    
    print(f"âœ… ë°ì´í„° ë¶„ë¦¬ ì„±ê³µ!")
    print(f"  Train: {len(train_df)}ê°œ")
    print(f"  Validation: {len(val_df)}ê°œ") 
    print(f"  Test: {len(test_df)}ê°œ")
    
    # ë¶„ë¦¬ í›„ ê° ì„¸íŠ¸ì˜ ë¼ë²¨ ë¶„í¬ í™•ì¸
    print(f"\nğŸ“Š ë¶„ë¦¬ í›„ ë¼ë²¨ ë¶„í¬:")
    for label in labels:
        train_count = len(train_df[train_df['label'] == label])
        val_count = len(val_df[val_df['label'] == label])
        test_count = len(test_df[test_df['label'] == label])
        total = train_count + val_count + test_count
        print(f"  {label}: Train({train_count}) + Val({val_count}) + Test({test_count}) = {total}")
    
except ValueError as e:
    print(f"âŒ ë°ì´í„° ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
    print("ë¼ë²¨ë³„ ìµœì†Œ ë°ì´í„° ìˆ˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

print("\n" + "="*50)
print("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# ===== ë°ì´í„° ì „ì²˜ë¦¬ =====
def encode_dataset(dataset):
    x_data, y_data = [], []
    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):
        text = row["text"]
        label_id = row["label_id"]
        ids, _ = tokenizer.encode(text, max_len=MAX_SEQ_LEN)
        x_data.append(ids)
        onehot = [0] * len(label2id)
        onehot[label_id] = 1
        y_data.append(onehot)
    return np.array(x_data), np.array(y_data)

train_x, train_y = encode_dataset(train_df)
val_x, val_y = encode_dataset(val_df)
test_x, test_y = encode_dataset(test_df)

# ===== ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ =====
print("\nëª¨ë¸ ì •ì˜ ì¤‘...")
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    input_ids = keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype='int32')
   
    bert_params = bert.params_from_pretrained_ckpt(os.path.dirname(checkpoint_path))
    l_bert = bert.BertModelLayer.from_params(bert_params, name="bert")
    bert_output = l_bert(input_ids)
    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)
    outputs = keras.layers.Dense(units=len(label2id), activation='softmax')(cls_out)

    model = keras.Model(inputs=input_ids, outputs=outputs)
    model.build(input_shape=(None, MAX_SEQ_LEN))
    bert.load_stock_weights(l_bert, checkpoint_path)

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LR, clipnorm=1.0),
        loss='categorical_crossentropy',
        metrics=['accuracy'],
    )

model.summary()


early_stop = EarlyStopping(
    monitor='val_loss',      # validation loss ê¸°ì¤€ìœ¼ë¡œ ê°ì‹œ
    patience=2,              # ê°œì„  ì•ˆ ë˜ëŠ” epoch ìˆ˜ (ex: 2ë²ˆ ì—°ì†)
    restore_best_weights=True  # ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ê°€ì¤‘ì¹˜ ë³µì›
)

# ===== í•™ìŠµ ì‹œì‘ =====
print("\nğŸ“š ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
try:
    history = model.fit(
        train_x, train_y,
        validation_data=(val_x, val_y),
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        callbacks=[early_stop],
        verbose=1
    )
except Exception as e:
    print("âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)

# ===== í‰ê°€ ë° ì €ì¥ =====
model.save("korpatBERT_patent_model.h5")
eval_result = model.evaluate(test_x, test_y)
print("\nğŸ“Š í‰ê°€ ê²°ê³¼:")
print("Accuracy: %.4f" % eval_result[1])
