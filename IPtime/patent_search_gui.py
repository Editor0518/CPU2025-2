import tkinter as tk
from tkinter import messagebox
import pandas as pd
import os

from patent_fetcher import fetch_patents
from korpat_tokenizer import Tokenizer

# ì„¤ì •
vocab_path = "./pretrained/korpat_vocab.txt"
max_len = 256

def process_text(text, tokenizer, max_len):
    """í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•"""
    if pd.isna(text) or text.strip() == "":
        return [0]*max_len, [0]*max_len, []
    input_ids, segment_ids = tokenizer.encode(text, max_len=max_len)
    tokens = tokenizer.tokenize(text)
    return input_ids, segment_ids, tokens

def run_program():
    search_word = entry_search.get().strip()
    withdrawn_flag = var_withdrawn.get()

    if not search_word:
        messagebox.showwarning("ì…ë ¥ ì˜¤ë¥˜", "ê²€ìƒ‰ì‹ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    if not os.path.exists(vocab_path):
        messagebox.showerror("ì˜¤ë¥˜", f"{vocab_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    text_output.delete("1.0", tk.END)
    text_output.insert(tk.END, "[INFO] KorPatBERT í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘...\n")

    tokenizer = Tokenizer(vocab_path=vocab_path, cased=True)
    text_output.insert(tk.END, "[OK] í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ\n")

    text_output.insert(tk.END, f"ğŸ” ê²€ìƒ‰ì–´: {search_word} | ì·¨í•˜ í¬í•¨: {withdrawn_flag}\n")
    text_output.insert(tk.END, "[INFO] íŠ¹í—ˆ ê²€ìƒ‰ ì¤‘...\n")

    try:
        df = fetch_patents(search_word=search_word, year_to_search="0", withdrawn=withdrawn_flag)
    except Exception as e:
        messagebox.showerror("ì—ëŸ¬", f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return

    if df.empty:
        text_output.insert(tk.END, "[WARN] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
        return

    text_output.insert(tk.END, f"[OK] {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ\n")
    text_output.insert(tk.END, "[INFO] í† í¬ë‚˜ì´ì§• ì§„í–‰ ì¤‘...\n")

    results = df['astrtCont'].fillna('').apply(lambda x: process_text(x, tokenizer, max_len))
    df['input_ids'] = results.apply(lambda x: x[0])
    df['segment_ids'] = results.apply(lambda x: x[1])
    df['tokens'] = results.apply(lambda x: x[2])

    output_file = "patents_tokenized.pkl"
    df.to_pickle(output_file)
    text_output.insert(tk.END, f"[OK] ì €ì¥ ì™„ë£Œ: {output_file}\n")

    # í†µê³„
    text_output.insert(tk.END, "\n=== í†µê³„ ===\n")
    token_lengths = df['tokens'].apply(len)
    text_output.insert(tk.END, f"ì´ íŠ¹í—ˆ ê±´ìˆ˜: {len(df)}\n")
    text_output.insert(tk.END, f"í‰ê·  í† í° ìˆ˜: {token_lengths.mean():.1f}\n")
    text_output.insert(tk.END, f"ìµœëŒ€ í† í° ìˆ˜: {token_lengths.max()}\n")
    text_output.insert(tk.END, f"ìµœì†Œ í† í° ìˆ˜: {token_lengths.min()}\n")

    # ìƒ˜í”Œ ì¶œë ¥
    first_patent = df.iloc[0]
    title = str(first_patent.get('inventionTitle', 'N/A'))[:50]
    content = str(first_patent.get('astrtCont', ''))
    tokens = first_patent['tokens']
    input_ids = first_patent['input_ids']

    text_output.insert(tk.END, "\n=== ìƒ˜í”Œ ===\n")
    text_output.insert(tk.END, f"ë°œëª…ëª…: {title}...\n")
    text_output.insert(tk.END, f"ì›ë¬¸ ê¸¸ì´: {len(content)}\n")
    text_output.insert(tk.END, f"í† í° ìˆ˜: {len(tokens)}\n")
    text_output.insert(tk.END, f"í† í° ì˜ˆì‹œ: {tokens[:10]}\n")
    text_output.insert(tk.END, f"input_ids ì˜ˆì‹œ: {input_ids[:10]}\n")

    text_output.insert(tk.END, "\n[ì™„ë£Œ] í† í¬ë‚˜ì´ì§•ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")

# GUI êµ¬ì„±
root = tk.Tk()
root.title("KIPRIS íŠ¹í—ˆ ìˆ˜ì§‘ + KorPatBERT í† í¬ë‚˜ì´ì§•")

# ê²€ìƒ‰ì‹ ì…ë ¥
tk.Label(root, text="ê²€ìƒ‰ì‹:").grid(row=0, column=0, sticky="e")
entry_search = tk.Entry(root, width=40)
entry_search.grid(row=0, column=1, padx=5, pady=5)

# ì˜µì…˜: ì·¨í•˜/ì†Œë©¸ í¬í•¨ ì—¬ë¶€
var_withdrawn = tk.BooleanVar()
chk_withdrawn = tk.Checkbutton(root, text="ì·¨í•˜/ì†Œë©¸/í¬ê¸° í¬í•¨", variable=var_withdrawn)
chk_withdrawn.grid(row=1, column=1, sticky="w", padx=5)

# ì‹¤í–‰ ë²„íŠ¼
btn_run = tk.Button(root, text="ì‹¤í–‰", command=run_program)
btn_run.grid(row=2, column=1, sticky="e", padx=5, pady=5)

# ì¶œë ¥ì°½
text_output = tk.Text(root, width=80, height=30)
text_output.grid(row=3, column=0, columnspan=2, padx=10, pady=10)

root.mainloop()
